from typing import Dict, List, Union, Any, Coroutine

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.prompts import PromptTemplate


async def chat_with_model_template_batch(model: BaseChatModel, prompt_template: PromptTemplate,
                                         variables_list: List[Dict[str, str]], return_str: bool = False) -> Union[
    List[str], List[Union[str, Dict[str, str]]], List[BaseMessage]]:
    """
    Batch call the language model to generate responses.

    Parameters:
        model (BaseChatModel): The language model instance to generate responses.
        prompt_template (PromptTemplate): The prompt template to format each request.
        variables_list (List[Dict[str, str]]): List of dictionaries with variables to inject into the PromptTemplate.
        return_str (bool): Whether to return responses as strings. Defaults to False.

    Returns:
        List[BaseMessage] or List[str]: A list of responses generated by the language model.
        Raises ValueError if responses are empty.
    """
    try:
        formatted_prompts = [prompt_template.format_prompt(**variables) for variables in variables_list]
    except KeyError as e:
        raise ValueError(f"Missing variable in prompt template formatting: {e}")

    if not formatted_prompts:
        raise ValueError("No formatted prompts generated.")

    responses = await model.abatch(inputs=formatted_prompts)

    if not responses or any(response is None for response in responses):
        raise ValueError("Generated responses are empty or contain None.")

    if return_str:
        return [response.content if isinstance(response, AIMessage) else str(response) for response in responses]

    return responses


async def chat_with_model_str(model: BaseChatModel, prompt: str, return_str: bool = False) -> \
        str | list[str | dict] | Coroutine[Any, Any, BaseMessage]:
    response = await model.ainvoke(prompt)

    if not response:
        raise ValueError("Generated response is empty, potential issue in chat_model processing.")

    if return_str:
        return response.content if isinstance(response, AIMessage) else str(response)

    return response

